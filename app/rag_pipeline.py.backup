import os
import logging
from pathlib import Path
from typing import Optional
from functools import lru_cache

from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate

# Configure logging
logger = logging.getLogger(__name__)

# Load environment variables
env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(dotenv_path=env_path)


# ---------- Configuration ----------
class Config:
    """Configuration for RAG pipeline."""

    # Embeddings
    EMBEDDING_MODEL = os.getenv(
        "EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2"
    )

    # Text Splitting
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 500))
    CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", 50))

    # Retrieval
    RETRIEVAL_K = int(os.getenv("RETRIEVAL_K", 3))
    SEARCH_TYPE = os.getenv("SEARCH_TYPE", "similarity")

    # LLM
    LLM_MODEL = os.getenv("LLM_MODEL", "google/flan-t5-base")
    LLM_MAX_LENGTH = int(os.getenv("LLM_MAX_LENGTH", 512))
    LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", 0.7))

    # OpenAI (optional)
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

    @classmethod
    def use_openai(cls) -> bool:
        """Check if OpenAI should be used."""
        return bool(cls.OPENAI_API_KEY)


# ---------- Cached Model Loading ----------
@lru_cache(maxsize=1)
def get_embeddings():
    """
    Return embeddings model (cached).
    Uses OpenAI if API key is set, otherwise HuggingFace (free).
    """
    try:
        if Config.use_openai():
            logger.info("âœ… Using OpenAI Embeddings")
            from langchain_openai import OpenAIEmbeddings

            return OpenAIEmbeddings(openai_api_key=Config.OPENAI_API_KEY)
        else:
            logger.info(f"âœ… Using HuggingFace Embeddings: {Config.EMBEDDING_MODEL}")
            return HuggingFaceEmbeddings(
                model_name=Config.EMBEDDING_MODEL,
                model_kwargs={"device": "cpu"},
                encode_kwargs={"normalize_embeddings": True},
            )
    except Exception as e:
        logger.error(f"Failed to load embeddings: {e}")
        raise RuntimeError(f"Embeddings initialization failed: {e}")


@lru_cache(maxsize=1)
def get_llm():
    """
    Return LLM model (cached).
    Uses OpenAI if API key is set, otherwise HuggingFace.
    """
    try:
        if Config.use_openai():
            logger.info(f"âœ… Using OpenAI: {Config.OPENAI_MODEL}")
            from langchain_openai import ChatOpenAI

            return ChatOpenAI(
                model=Config.OPENAI_MODEL,
                openai_api_key=Config.OPENAI_API_KEY,
                temperature=Config.LLM_TEMPERATURE,
            )
        else:
            logger.info(f"âœ… Using HuggingFace LLM: {Config.LLM_MODEL}")
            from transformers import pipeline
            from langchain_community.llms import HuggingFacePipeline

            hf_pipeline = pipeline(
                "text2text-generation",
                model=Config.LLM_MODEL,
                max_length=Config.LLM_MAX_LENGTH,
                temperature=Config.LLM_TEMPERATURE,
                device=-1,  # CPU
            )
            return HuggingFacePipeline(pipeline=hf_pipeline)
    except Exception as e:
        logger.error(f"Failed to load LLM: {e}")
        raise RuntimeError(f"LLM initialization failed: {e}")


# ---------- Vector Store Functions ----------
def build_vector_store(pdf_path: str, store_path: str) -> FAISS:
    """
    Load PDF, split into chunks, embed, and save FAISS index.

    Args:
        pdf_path: Path to PDF file
        store_path: Path to save vector store

    Returns:
        FAISS vector store

    Raises:
        FileNotFoundError: If PDF doesn't exist
        RuntimeError: If building fails
    """
    # Validate PDF path
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"PDF not found: {pdf_path}")

    try:
        logger.info(f"ðŸ“„ Loading PDF: {pdf_path}")
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()

        if not docs:
            raise ValueError("No documents loaded from PDF")

        logger.info(f"âœ… Loaded {len(docs)} pages")

        # Split documents
        logger.info(
            f"âœ‚ï¸ Splitting into chunks (size={Config.CHUNK_SIZE}, overlap={Config.CHUNK_OVERLAP})"
        )
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", " ", ""],
        )
        chunks = splitter.split_documents(docs)
        logger.info(f"âœ… Created {len(chunks)} chunks")

        # Create embeddings and build vector store
        logger.info("ðŸ”¢ Creating embeddings...")
        embeddings = get_embeddings()
        vector_store = FAISS.from_documents(chunks, embeddings)

        # Save vector store
        logger.info(f"ðŸ’¾ Saving vector store to: {store_path}")
        os.makedirs(os.path.dirname(store_path), exist_ok=True)
        vector_store.save_local(store_path)
        logger.info("âœ… Vector store saved successfully")

        return vector_store

    except Exception as e:
        logger.error(f"Failed to build vector store: {e}")
        raise RuntimeError(f"Vector store creation failed: {e}")


def load_vector_store(store_path: str) -> FAISS:
    """
    Load existing FAISS vector store.

    Args:
        store_path: Path to vector store directory

    Returns:
        FAISS vector store

    Raises:
        FileNotFoundError: If store doesn't exist
        RuntimeError: If loading fails
    """
    if not os.path.exists(store_path):
        raise FileNotFoundError(
            f"Vector store not found at: {store_path}\n"
            f"Please run 'python run_me_once.py' first to create it."
        )

    try:
        logger.info(f"ðŸ“š Loading vector store from: {store_path}")
        embeddings = get_embeddings()

        # SECURITY FIX: Removed allow_dangerous_deserialization
        vector_store = FAISS.load_local(
            store_path,
            embeddings,
            allow_dangerous_deserialization=False,  # âœ… SAFE
        )
        logger.info("âœ… Vector store loaded successfully")
        return vector_store

    except Exception as e:
        logger.error(f"Failed to load vector store: {e}")
        raise RuntimeError(f"Vector store loading failed: {e}")


# ---------- QA Chain ----------
def make_qa_chain(vector_store: FAISS) -> RetrievalQA:
    """
    Build RetrievalQA chain with custom prompt.

    Args:
        vector_store: FAISS vector store

    Returns:
        RetrievalQA chain
    """
    try:
        # Create retriever
        retriever = vector_store.as_retriever(
            search_type=Config.SEARCH_TYPE, search_kwargs={"k": Config.RETRIEVAL_K}
        )

        # Custom prompt template
        prompt_template = """Use the following pieces of context to answer the question at the end. 
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Keep your answer concise and relevant.

Context:
{context}

Question: {question}

Helpful Answer:"""

        PROMPT = PromptTemplate(
            template=prompt_template, input_variables=["context", "question"]
        )

        # Get LLM
        llm = get_llm()

        # Build chain
        logger.info("ðŸ”— Building QA chain...")
        chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=False,
        )

        logger.info("âœ… QA chain built successfully")
        return chain

    except Exception as e:
        logger.error(f"Failed to create QA chain: {e}")
        raise RuntimeError(f"QA chain creation failed: {e}")


# ---------- Helper Functions ----------
def get_vector_store_info(store_path: str) -> dict:
    """
    Get information about vector store.

    Args:
        store_path: Path to vector store

    Returns:
        Dictionary with store information
    """
    if not os.path.exists(store_path):
        return {"exists": False}

    try:
        vector_store = load_vector_store(store_path)
        return {
            "exists": True,
            "num_documents": vector_store.index.ntotal,
            "path": store_path,
        }
    except Exception as e:
        return {"exists": True, "error": str(e)}
